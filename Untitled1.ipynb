{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCKhfZ7frmodxbjAY4tL0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anagha235/CNN-Data-Science-Project/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "rPRwoyEcXSUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLMAco_QW2r-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomBrightness, RandomCrop\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from collections import Counter\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "import keras_tuner\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N90KjKMJXYa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = r'/content/drive/MyDrive/RealWaste'\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    image_size=(224, 224),\n",
        "\n",
        "\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "KfuSLuN7XajW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = sorted(os.listdir(data_path))\n",
        "num_classes = len(class_names)\n",
        "class_counts = np.zeros(num_classes)"
      ],
      "metadata": {
        "id": "lX8tRxUVYt27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    RandomFlip(\"horizontal_and_vertical\"),\n",
        "    RandomRotation(0.2),\n",
        "    RandomZoom(0.2),\n",
        "    RandomBrightness(0.2),\n",
        "    RandomCrop(224, 224),\n",
        "])\n",
        "\n",
        "def augment(image, label):\n",
        "\n",
        "    image = data_augmentation(image)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    return image , label\n"
      ],
      "metadata": {
        "id": "SOUPdxzBXcgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_dataset(dataset, class_names):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for images, labels in dataset.take(1):\n",
        "          image = images[0]\n",
        "\n",
        "\n",
        "    augmented_images = [image]\n",
        "    for _ in range(8):\n",
        "        augmented_images.append(data_augmentation(image[tf.newaxis, ...])[0])\n",
        "\n",
        "\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        img = augmented_images[i].numpy().astype(\"uint8\")\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle('Augmented Image')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_dataset(dataset, class_names)"
      ],
      "metadata": {
        "id": "gSE6ykgnYkfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalize_and_encode(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    label = tf.cast(label, tf.int32)\n",
        "    label = tf.one_hot(label, num_classes)\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "4jp_SMUnYohz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = tf.data.experimental.cardinality(dataset).numpy()"
      ],
      "metadata": {
        "id": "XhYDqi9AYu_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_size = int(0.7 * dataset_size)\n",
        "val_size = int(0.15 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\n",
        "\n",
        "train_dataset = dataset.take(train_size).map(augment).map(normalize_and_encode)\n",
        "remaining_dataset = dataset.skip(train_size)\n",
        "val_dataset = remaining_dataset.take(val_size).map(normalize_and_encode)\n",
        "test_dataset = remaining_dataset.skip(val_size).map(normalize_and_encode)"
      ],
      "metadata": {
        "id": "OjcuovtoYx1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_label(label):\n",
        "    class_index = tf.argmax(label).numpy()\n",
        "    return class_names[class_index]\n",
        "\n",
        "def denormalize(image):\n",
        "    image = image * 255\n",
        "    image = tf.cast(image, tf.uint8)\n",
        "    return image\n",
        "\n",
        "def visualize_train_dataset(train_dataset, num_images=9):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    for images, labels in train_dataset.take(1):\n",
        "        for i in range(min(num_images, len(images))):\n",
        "            plt.subplot(3, 3, i + 1)\n",
        "            img = denormalize(images[i])\n",
        "            plt.imshow(img.numpy())\n",
        "            plt.title(f\"Label: {decode_label(labels[i])}\")\n",
        "            plt.axis(\"off\")\n",
        "    plt.suptitle(\"Sample Images from Training Dataset\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_train_dataset(train_dataset)"
      ],
      "metadata": {
        "id": "TQNzMOFwY1pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_counts = {class_name: len(os.listdir(os.path.join(data_path, class_name)))\n",
        "                for class_name in class_names}\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color='blue')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Class\", fontsize = 15 )\n",
        "plt.ylabel(\"Number of Images\", fontsize = 15 )\n",
        "plt.title(\"Class Distribution in Dataset\", fontsize = 18 )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qHhPR8zSY2fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_labels = []\n",
        "\n",
        "\n",
        "for batch in train_dataset.as_numpy_iterator():\n",
        "    labels = batch[1]\n",
        "\n",
        "\n",
        "    if labels.ndim > 1:\n",
        "        labels = np.argmax(labels, axis=1)\n",
        "\n",
        "    train_labels.extend(labels)\n",
        "\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "\n",
        "print(\"Class weights:\", class_weights_dict)\n"
      ],
      "metadata": {
        "id": "3efJjG0mY6LF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}