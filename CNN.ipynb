{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVv1tyFj-P7b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomBrightness, RandomCrop\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from collections import Counter\n",
        "from tensorflow.keras import backend as K\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uRwfvo55U-pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swXkzDoi-P7i"
      },
      "outputs": [],
      "source": [
        "data_path = r'/content/drive/MyDrive/RealWaste'\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = sorted(os.listdir(data_path))  # Get folder names\n",
        "num_classes = len(class_names)\n",
        "class_counts = np.zeros(num_classes)"
      ],
      "metadata": {
        "id": "7GYCyg-0Q--a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a9JqX4T-P7k"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    RandomFlip(\"horizontal\"),   # Randomly flip images horizontally\n",
        "    tf.keras.layers.RandomRotation(0.4),         # Randomly rotate images by up to 20%\n",
        "    RandomZoom(0.4),             # Randomly zoom into images by 20%\n",
        "    RandomBrightness(factor=0.4),        # Randomly increase the brightness\n",
        "    RandomCrop(150,160)\n",
        "])\n",
        "\n",
        "\n",
        "# Apply augmentation to the training data\n",
        "def augment(image, label):\n",
        "\n",
        "    image = data_augmentation(image)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    return image , label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPV3oUfy-P7l"
      },
      "outputs": [],
      "source": [
        "def visualize_dataset(dataset, class_names):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for images, labels in dataset.take(1):\n",
        "          image = images[0]  # Select the first image in the batch\n",
        "\n",
        "        # Apply augmentation multiple times to the same image\n",
        "    augmented_images = [image]\n",
        "    for _ in range(8):  # Create 8 augmented versions\n",
        "        augmented_images.append(data_augmentation(image[tf.newaxis, ...])[0])\n",
        "\n",
        "        # Display the original and augmented images\n",
        "    for i in range(9):  # Display 9 images (1 original + 8 augmented)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        img = augmented_images[i].numpy().astype(\"uint8\")\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to visualize data augmentation\n",
        "visualize_dataset(dataset, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ns5gVMl-P7n"
      },
      "outputs": [],
      "source": [
        "def normalize_img(image, label):\n",
        "    image = image/ 255.0  # Scale pixel values to [0, 1]\n",
        "    label = tf.cast(label, tf.int32)\n",
        "    label = tf.one_hot(label, num_classes)\n",
        "    return image, label\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9kuKkhY-P7n"
      },
      "outputs": [],
      "source": [
        "dataset_size = tf.data.experimental.cardinality(dataset).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7 * dataset_size)  # 70% for training\n",
        "val_size = int(0.15 * dataset_size)   # 15% for validation\n",
        "test_size = dataset_size - train_size - val_size  # 15% for testing\n",
        "\n",
        "train_dataset = dataset.take(train_size).map(augment).map(normalize_img)\n",
        "\n",
        "val_dataset = dataset.skip(train_size).take(val_size).map(normalize_img)\n",
        "test_dataset = dataset.skip(train_size + val_size).map(normalize_img)"
      ],
      "metadata": {
        "id": "RyUOV9uXRCEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWTEeVnd-P7q"
      },
      "outputs": [],
      "source": [
        "for images, labels in train_dataset.take(1):\n",
        "    print(f\"Min pixel value: {tf.reduce_min(images).numpy()}\")\n",
        "    print(f\"Max pixel value: {tf.reduce_max(images).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3W0Twv7-P7q"
      },
      "outputs": [],
      "source": [
        "def visualize_dataset(dataset, class_names):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for images, labels in dataset.take(1):  # Get one batch\n",
        "        for i in range(9):  # Show 9 images\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            img = images[i].numpy()\n",
        "            plt.imshow(img)\n",
        "            label_idx = tf.argmax(labels[i]).numpy()  # Extract class index from one-hot label\n",
        "            plt.title(class_names[label_idx])\n",
        "            plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize data augmentation results\n",
        "visualize_dataset(train_dataset, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOuVNX7i-P7r"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Count number of images in each class\n",
        "class_counts = {class_name: len(os.listdir(os.path.join(data_path, class_name)))\n",
        "                for class_name in class_names}\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color='blue')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Class\", fontsize = 15 )\n",
        "plt.ylabel(\"Number of Images\", fontsize = 15 )\n",
        "plt.title(\"Class Distribution in Dataset\", fontsize = 18 )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_counter = Counter()\n",
        "for _, label in train_dataset.unbatch().as_numpy_iterator():\n",
        "    label = np.array(label)\n",
        "    if label.size == 1:\n",
        "        label = label.item()\n",
        "    else:\n",
        "        label = int(np.argmax(label))\n",
        "    label_counter[int(label)] += 1\n",
        "\n",
        "# Step 4: Identify underrepresented classes (e.g., below median)\n",
        "counts = np.array(list(label_counter.values()))\n",
        "median_count = np.median(counts)\n",
        "low_classes = [cls for cls, count in label_counter.items() if count < median_count]\n",
        "low_class_tensor = tf.constant(low_classes, dtype=tf.int64)\n",
        "print(\"Low-frequency classes:\", low_classes)\n",
        "\n",
        "def is_low_class(image, label):\n",
        "    label = tf.cast(tf.argmax(label, axis=-1), tf.int64)\n",
        "    return tf.reduce_any(tf.equal(label, low_class_tensor))\n",
        "\n",
        "def extra_augment(image, label):\n",
        "    image = tf.image.rot90(image)\n",
        "    image = tf.image.random_saturation(image, 0.6, 1.4)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    label = tf.one_hot(tf.cast(label, tf.int32), 9)\n",
        "    return image, label\n",
        "\n",
        "low_class_augmented = train_dataset.unbatch().filter(is_low_class).map(extra_augment).map(normalize_img).repeat(3).batch(32)\n",
        "\n",
        "# Step 6: Combine both into final training set\n",
        "\n",
        "\n",
        "final_train_dataset = train_dataset.concatenate(low_class_augmented).unbatch().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "xsKgDX9Jj7gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class distribution after final augmentation:\")\n",
        "label_counter = Counter()\n",
        "\n",
        "for _, labels in final_train_dataset.unbatch().take(1000):\n",
        "    label = labels.numpy()\n",
        "    if label.shape == (9,):  # one-hot\n",
        "        label_idx = np.argmax(label)\n",
        "        label_counter[int(label_idx)] += 1\n",
        "\n",
        "print(dict(label_counter))\n",
        "\n"
      ],
      "metadata": {
        "id": "fBm0AK17qtgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(label_counter.keys(), label_counter.values(), color='blue')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Class\", fontsize = 15 )\n",
        "plt.ylabel(\"Number of Images\", fontsize = 15 )\n",
        "plt.title(\"Class Distribution in Dataset\", fontsize = 18 )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K0nRPZ-43MJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class weights computation\n",
        "train_labels = []\n",
        "for _, label in final_train_dataset.unbatch().take(1000):  # Limit for speed/safety\n",
        "    label = label.numpy()\n",
        "    while label.ndim > 1:\n",
        "        label = np.mean(label, axis=0)\n",
        "    if label.ndim == 1 and label.shape[0] == 9:\n",
        "        class_idx = int(np.argmax(label))\n",
        "        train_labels.append(class_idx)\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.arange(9), y=train_labels)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weights_dict)\n"
      ],
      "metadata": {
        "id": "1lDLqjDj253l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def categorical_focal_loss(gamma=2.0, alpha=0.25):\n",
        "    \"\"\"\n",
        "    Focal loss for multi-class classification with one-hot labels.\n",
        "    \"\"\"\n",
        "    def loss(y_true, y_pred):\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        cross_entropy = -y_true * K.log(y_pred)\n",
        "        weight = alpha * K.pow(1 - y_pred, gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return K.sum(loss, axis=1)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "EztGqPEsY-JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O3ZwIyek-P7s"
      },
      "outputs": [],
      "source": [
        "base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dense(9, activation=\"softmax\")  # Adjust for the number of classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=categorical_focal_loss(gamma=2.0, alpha=0.25),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\",  # Monitor the validation loss\n",
        "                               patience=3,  # Number of epochs to wait for improvement\n",
        "                               restore_best_weights=True,  # Restore the best model weights when stopping\n",
        "                               verbose=1)  # Print messages when stopping\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=20, class_weight=class_weights_dict, callbacks=[early_stopping])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning (optional)\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:20]:  # Freeze first 100 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile after unfreezing\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
        "              loss=categorical_focal_loss(gamma=2.0, alpha=0.25),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train again on the small batch\n",
        "history_fine_tune = model.fit(train_dataset, validation_data=val_dataset, epochs=20, class_weight=class_weights_dict, )"
      ],
      "metadata": {
        "id": "vaByECRSi6uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get true labels and predictions\n",
        "y_true = []\n",
        "y_pred_probs = []\n",
        "\n",
        "for images, labels in val_dataset:\n",
        "    preds = model.predict(images)  # Get predicted probabilities\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))  # Convert one-hot labels to class indices\n",
        "    y_pred_probs.extend(preds)  # Store predicted probabilities\n",
        "\n",
        "y_pred_probs = np.array(y_pred_probs)\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)"
      ],
      "metadata": {
        "id": "rSxFev13MZB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "ZI63J0fBNK8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hist(history_fine_tune):\n",
        "    plt.plot(history_fine_tune.history[\"accuracy\"])\n",
        "    plt.plot(history_fine_tune.history[\"val_accuracy\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_hist(history_fine_tune)"
      ],
      "metadata": {
        "id": "3d-iUS_lNf2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get true labels and predictions\n",
        "y_true = []\n",
        "y_pred_probs = []\n",
        "\n",
        "for images, labels in val_dataset:\n",
        "    preds = model.predict(images)  # Get predicted probabilities\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))  # Convert one-hot labels to class indices\n",
        "    y_pred_probs.extend(preds)  # Store predicted probabilities\n",
        "\n",
        "y_pred_probs = np.array(y_pred_probs)\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n"
      ],
      "metadata": {
        "id": "Qqk0CPnNThOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "lKrYHnORavPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert true labels to one-hot encoding\n",
        "num_classes = 9  # Adjust to your number of classes\n",
        "y_true_one_hot = to_categorical(y_true, num_classes)\n",
        "\n",
        "# Plot ROC Curve for each class\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i in range(num_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_true_one_hot[:, i], y_pred_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "# Plot baseline\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e4jdwEaXbCkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HdcGyzgLbjfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test dataset\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "G6TGBZkrcb9-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}